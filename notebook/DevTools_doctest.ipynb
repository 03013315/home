{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Testing through documentation\n",
    "\n",
    "26.3 doctest — Test interactive Python examples   \n",
    "https://docs.python.org/3/library/doctest.html\n",
    "\n",
    "doctest – Testing through documentation   \n",
    "https://pymotw.com/2/doctest/index.html\n",
    "\n",
    "<b>doctest</b> lets you <b>test</b> your code by running <b>examples embedded in the documentation</b> and verifying that they produce the expected results. \n",
    "\n",
    "It works by parsing the help text to find examples, running them, then comparing the output text against the expected value. \n",
    "\n",
    "Many developers find doctest <b>easier</b> than unittest because in its simplest form, there is no API to learn before using it.\n",
    "\n",
    "However, as the examples become more complex <b>the lack of fixture management</b> can make writing doctest tests more <b>cumbersome</b> than using unittest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Getting Started\n",
    "\n",
    "<b>doctest</b> looks for lines <b>beginning</b> with \n",
    "\n",
    "the interpreter prompt, <b>>>></b>, to find the beginning of a test case. \n",
    "\n",
    "The case is <b>ended</b> \n",
    "      \n",
    "       by <b>a blank line</b>, \n",
    "      \n",
    "      or by the <b>next interpreter prompt</b>.\n",
    "\n",
    "Here, <b>my_function()</b> has two examples given in the module: doctest_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_function(a, b):\n",
    "    \"\"\"\n",
    "    >>> my_function(2, 3)\n",
    "    6\n",
    "    >>> my_function('a', 3)\n",
    "    'aaa'\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the tests, use <b>doctest as the main program</b> via the <b>-m</b> option to the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest ..\\code\\doctest\\doctest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually no output is produced while the tests are running,\n",
    "\n",
    "so the example below includes the <b>-v</b> option to make the output more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest -v ..\\code\\doctest\\doctest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples cannot usually stand on their own as explanations of a function, so doctest also lets you keep the surrounding text you would normally include in the documentation. \n",
    "\n",
    "Intervening text is ignored, and can have any format as long as it does not look like a test case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_function(a, b):\n",
    "    \"\"\"Returns a * b.\n",
    "\n",
    "    Works with numbers:\n",
    "    \n",
    "    >>> my_function(2, 3)\n",
    "    6\n",
    "\n",
    "    and strings:\n",
    "    \n",
    "    >>> my_function('a', 3)\n",
    "    'aaa'\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surrounding text in the updated docstring makes it more <b>useful to a human reader</b>, and is  <b>ignored by doctest</b>, and the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest -v ..\\code\\doctest\\doctest_simple_with_docs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Handling Unpredictable Output\n",
    "\n",
    "There are other cases where the <b>exact output may not be predictable</b>, but should still be testable.\n",
    "\n",
    "* Local date and time values and object ids <b>change</b> on every test run. \n",
    "\n",
    "* The default precision used in the representation of floating point values depend on compiler options.\n",
    "\n",
    "* Object string representations may not be deterministic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    pass\n",
    "\n",
    "def unpredictable(obj):\n",
    "    \"\"\"Returns a new list containing obj.\n",
    "\n",
    "    >>> unpredictable(MyClass())\n",
    "    [<doctest_unpredictable.MyClass object at 0x10055a2d0>]\n",
    "    \"\"\"\n",
    "    return [obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `id` values change each time a program runs, because it is loaded into a different part of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest -v ..\\code\\doctest\\doctest_unpredictable.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When the tests include values that are likely to <b>change in unpredictable ways</b>, and where the actual value is not important to the test results,\n",
    "\n",
    "you can use the <b>ELLIPSIS</b> option to tell `doctest` to ignore portions of the verification value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The comment after the call to `unpredictable()` (#doctest: +ELLIPSIS) tells `doctest` to turn on the ELLIPSIS option for that test. \n",
    "The `...` replaces `the memory address` in the object id, so that portion of the expected value is ignored and the actual output matches and the test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest -v ..\\code\\doctest\\doctest_ellipsis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are cases where you cannot ignore the unpredictable value, because that would obviate the test.\n",
    "\n",
    "For example, simple tests quickly become more complex when dealing with data types whose string representations are inconsistent. \n",
    "\n",
    "The string form of a dictionary, for example, may <b>change based on the order the keys are added<b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = [ 'a', 'aa', 'aaa' ]\n",
    "\n",
    "d1 = dict( (k,len(k)) for k in keys )\n",
    "\n",
    "d2 = dict( (k,len(k)) for k in reversed(keys) )\n",
    "\n",
    "print\n",
    "print('d1:', d1)\n",
    "print('d2:', d2)\n",
    "print('d1 == d2:', d1 == d2)\n",
    "\n",
    "s1 = set(keys)\n",
    "s2 = set(reversed(keys))\n",
    "\n",
    "print\n",
    "print('\\ns1:', s1)\n",
    "print('s2:', s2)\n",
    "print('s1 == s2:', s1 == s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of <b>cache collision</b>, the internal <b>key list order is different</b> for the two dictionaries, evencthough they contain the same values and are considered to be equal.\n",
    "\n",
    "<b>Sets</b> use the same hashing algorithm, and exhibit the same behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best way to deal with these potential discrepancies is\n",
    "\n",
    "to create tests that produce values that are not likely to change. \n",
    "\n",
    "In the case of `dictionaries` and `sets`, that might mean looking for <b>specific keys</b> individually, generating a sorted list of the contents of the data structure, or comparing against <b>a literal value</b> for equality instead of depending on <b>the string representation</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by_length(words):\n",
    "    \"\"\"Returns a dictionary grouping words into sets by length.\n",
    "\n",
    "    >>> grouped = group_by_length([ 'python', 'module', 'of', 'the', 'week' ])\n",
    "    >>> grouped == { 2:set(['of']),\n",
    "    ...              3:set(['the']),\n",
    "    ...              4:set(['week']),\n",
    "    ...              6:set(['python', 'module']),\n",
    "    ...              }\n",
    "    True\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for word in words:\n",
    "        s = d.setdefault(len(word), set())\n",
    "        s.add(word)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the single example is actually interpreted as two separate tests, with \n",
    "\n",
    "the first expecting no console output and \n",
    "\n",
    "the second expecting the boolean result of the comparison operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m doctest -v ..\\code\\doctest\\doctest_hashed_values_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracebacks are a special case of changing data. Since the paths in a traceback depend on the location where a module is installed on the filesystem on a given system, it would be impossible to write portable tests if they were treated the same as other output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def this_raises():\n",
    "    \"\"\"This function always raises an exception.\n",
    "\n",
    "    >>> this_raises()\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "      File \"/no/such/path/doctest_tracebacks.py\", line 14, in this_raises\n",
    "        raise RuntimeError('here is the error')\n",
    "    RuntimeError: here is the error\n",
    "    \"\"\"\n",
    "    raise RuntimeError('here is the error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doctest makes a special effort to recognize tracebacks, and ignore the parts that might change from system to system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " !python -m doctest -v ..\\code\\doctest\\doctest_tracebacks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

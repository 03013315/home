{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26.4. unittest — Unit testing framework\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "Python’s <b>unittest</b> module, sometimes referred to as `PyUnit`, is based on the XUnit framework design by Kent Beck and Erich Gamma. The same pattern is repeated in many other languages, including C, perl, Java, and Smalltalk. The framework implemented by unittest supports fixtures, test suites, and a test runner to enable automated testing for your code.\n",
    "\n",
    "\n",
    "### Basic Test Structure\n",
    "\n",
    "Tests, as defined by <b>unittest</b>, have two parts: \n",
    "\n",
    "<b>code to manage test “fixtures”</b>\n",
    "\n",
    "<b>the test itself</b>. \n",
    "\n",
    "Individual tests are created by subclassing `TestCase` and overriding or adding appropriate methods. \n",
    "\n",
    "For example: unittest_simple.py\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test(self):\n",
    "        self.asserTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the <b>SimplisticTest</b> has a single <b>test()</b> method, which would <b>fail if True is ever False</b>.\n",
    "\n",
    "## Running Tests\n",
    "\n",
    "The easiest way to run unittest tests is to include:\n",
    "```\n",
    "    if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "at the bottom of each test file, then simply run the script directly from the command line:\n",
    "```    \n",
    "   >python unittest_simple.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This abbreviated output includes <b>the amount of time the tests took</b>, along with <b>a status indicator for each test</b> (the ”.” on the first line of output means that a test passed).\n",
    "<img src=\"./img/unittest_simple.PNG\"/> \n",
    "\n",
    "For more <b>detailed test</b> results, include the <b>-v</b> option:\n",
    "```\n",
    ">python unittest_simple.py -v\n",
    "```\n",
    "<img src=\"./img/unittest_simple_v.PNG\"/> \n",
    "\n",
    "## Test Outcomes\n",
    "\n",
    "Tests have 3 possible outcomes:\n",
    "\n",
    "1. <b>ok</b>: The test passes\n",
    "\n",
    "2. <b>FAIL</b>:The test does not pass, and raises an AssertionError exception.\n",
    "\n",
    "3. <b>ERROR</b>: The test raises an exception other than AssertionError.\n",
    "\n",
    "There is no explicit way to <b>cause</b> a test to “pass”, so a test’s status depends on the presence (or absence) of an exception.\n",
    "\n",
    "For Example: unittest_outcomes.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class OutcomesTest(unittest.TestCase):\n",
    "\n",
    "    #   ok\n",
    "    def testPass(self):\n",
    "        return\n",
    "\n",
    "    # FAIL\n",
    "    def testFail(self):\n",
    "        self.assertFalse(True)\n",
    "    # ERROR\n",
    "    def testError(self):\n",
    "        raise RuntimeError('Test error!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When a test fails or generates an error, the <b>traceback</b> is included in the output.\n",
    "```\n",
    "EF.\n",
    "======================================================================\n",
    "ERROR: testError (__main__.OutcomesTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_outcomes.py\", line 12, in testError\n",
    "    raise RuntimeError('Test error!')\n",
    "RuntimeError: Test error!\n",
    "\n",
    "======================================================================\n",
    "FAIL: testFail (__main__.OutcomesTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_outcomes.py\", line 9, in testFail\n",
    "    self.assertFalse(True)\n",
    "AssertionError: True is not false\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 3 tests in 0.015s\n",
    "\n",
    "FAILED (failures=1, errors=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the example above, <b>testFail()</b> fails and the traceback <b>shows the line</b> with the failure code. It is up to the person reading the test output to look at the code to figure out the semantic meaning of the failed test, though. \n",
    "\n",
    "To make it <b>easier to understand the nature of a test failure<b>, the <b>fail*() and assert*()</b> methods all accept an argument <b>msg</b>, which can be used to produce <b>a more detailed error message</b>\n",
    "\n",
    "Example: unittest_failwithmessage.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class FailureMessageTest(unittest.TestCase):\n",
    "\n",
    "    def testFail(self):\n",
    "        self.assertFalse(True, 'failure message goes here')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "F\n",
    "======================================================================\n",
    "FAIL: testFail (__main__.FailureMessageTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_failwithmessage.py\", line 6, in testFail\n",
    "    self.assertFalse(True, 'failure message goes here')\n",
    "AssertionError: True is not false : failure message goes here\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.008s\n",
    "\n",
    "FAILED (failures=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asserting Truth\n",
    "\n",
    "Most tests assert the truth of some condition. There are a few different ways to write truth-checking tests, depending on the perspective of the test author and the desired outcome of the code being tested. \n",
    "\n",
    "If the code produces a value which can be evaluated as <b>true</b>, the methods <b>assertTrue()</b>  should be used.\n",
    "\n",
    "If the code produces a <b>false</b> value, the methods <b>assertFalse()</b> make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TruthTest(unittest.TestCase):\n",
    "\n",
    "    def testAssertTrue(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def testAssertFalse(self):\n",
    "        self.assertFalse(False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Equality\n",
    "\n",
    "As a special case, `unittest` includes methods for testing <b>the equality of two values</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mport unittest\n",
    "\n",
    "class EqualityTest(unittest.TestCase):\n",
    "\n",
    "    def testEqual(self):\n",
    "        self.assertEqual(1, 3-2)\n",
    "\n",
    "    def testNotEqual(self):\n",
    "        self.assertNotEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "..\n",
    "----------------------------------------------------------------------\n",
    "Ran 2 tests in 0.059s\n",
    "\n",
    "OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special tests are handy, since the values being <b>compared appear in the failure message</b> when a test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class InequalityTest(unittest.TestCase):\n",
    "\n",
    "    def testEqual(self):\n",
    "        self.assertNotEqual(1, 3-2)\n",
    "\n",
    "    def testNotEqual(self):\n",
    "        self.assertEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when these tests are run:\n",
    "```\n",
    "FF\n",
    "======================================================================\n",
    "FAIL: testEqual (__main__.InequalityTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_notequal.py\", line 6, in testEqual\n",
    "    self.assertNotEqual(1, 3-2)\n",
    "AssertionError: 1 == 1\n",
    "\n",
    "======================================================================\n",
    "FAIL: testNotEqual (__main__.InequalityTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_notequal.py\", line 9, in testNotEqual\n",
    "    self.assertEqual(2, 3-2)\n",
    "AssertionError: 2 != 1\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 2 tests in 0.059s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost Equal?\n",
    "\n",
    "In addition to strict equality, it is possible to test for <b>near equality of floating point numbers<b> using assertNotAlmostEqual() and assertAlmostEqual().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class AlmostEqualTest(unittest.TestCase):\n",
    "\n",
    "    def testNotAlmostEqual(self):\n",
    "        self.assertNotAlmostEqual(1.1, 3.3-2.0, places=1)\n",
    "\n",
    "    def testAlmostEqual(self):\n",
    "        self.assertAlmostEqual(1.1, 3.3-2.0, places=0)\n",
    "\n",
    "    def testAlmostEqualWithDefault(self):\n",
    "        self.assertAlmostEqual(1.1, 3.3-2.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are the values to be compared, and <b>the number of decimal places</b> to use for the test.\n",
    "\n",
    "assertAlmostEquals() and assertNotAlmostEqual  have an optional parameter named <b>places</b> and the numbers are compared by <b>computing the difference rounded to number of decimal places</b>.\n",
    "\n",
    "By default <b>places=7</b>, hence \n",
    "\n",
    "self.assertAlmostEqual(0.5, 0.4) is False \n",
    "\n",
    "while \n",
    "\n",
    "self.assertAlmostEqual(0.12345678, 0.12345679) is True.\n",
    "\n",
    "```\n",
    ".F.\n",
    "======================================================================\n",
    "FAIL: testAlmostEqualWithDefault (__main__.AlmostEqualTest)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"D:/SEUCourse/SE_ThermalEnergy/git/Py03013050/code/Chapter 26.4/unittest_almostequal.py\", line 12, in testAlmostEqualWithDefault\n",
    "    self.assertAlmostEqual(1.1, 3.3-2.0)\n",
    "AssertionError: 1.1 != 1.2999999999999998 within 7 places\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 3 tests in 0.014s\n",
    "\n",
    "FAILED (failures=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Exceptions\n",
    "\n",
    "As previously mentioned, if a test raises an exception other than AssertionError it is treated as an error. This is very useful for uncovering mistakes while you are modifying code which has existing test coverage. There are circumstances, however, in which you want the test to verify that some code does produce an exception. For example, if an invalid value is given to an attribute of an object. In such cases, failUnlessRaises() makes the code more clear than trapping the exception yourself. Compare these two tests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
